{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tabular_dnn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOcFZ+lwAr/ynPlTuWinlTS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeawoo0594/colabproject/blob/main/tabular_dnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXbHocmiKBiM"
      },
      "source": [
        "import seaborn as sns\r\n",
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib.offsetbox import AnchoredText\r\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucOyZcEzaP2p"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive//tabular/train.csv')\r\n",
        "test = pd.read_csv('/content/drive/My Drive//tabular/test.csv')\r\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfuKAp7ywo7J"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "import timeit\r\n",
        "\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  print(\r\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\r\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\r\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "\r\n",
        "def cpu():\r\n",
        "  with tf.device('/cpu:0'):\r\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\r\n",
        "    return tf.math.reduce_sum(net_cpu)\r\n",
        "\r\n",
        "def gpu():\r\n",
        "  with tf.device('/device:GPU:0'):\r\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\r\n",
        "    return tf.math.reduce_sum(net_gpu)\r\n",
        "  \r\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\r\n",
        "cpu()\r\n",
        "gpu()\r\n",
        "\r\n",
        "# Run the op several times.\r\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\r\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\r\n",
        "print('CPU (s):')\r\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\r\n",
        "print(cpu_time)\r\n",
        "print('GPU (s):')\r\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\r\n",
        "print(gpu_time)\r\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLS7eQA8y5J-"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd85kZFrArho"
      },
      "source": [
        "train['cat9'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REfmQYHgAxHb"
      },
      "source": [
        "test['cat9'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhM1NQGjBAH1"
      },
      "source": [
        "for category,proportion in train['cat9'].value_counts(normalize=True).iteritems():\r\n",
        "    print(f'cate: {category}, pro: {proportion}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvEq0f3ABRGH"
      },
      "source": [
        "for category,proportion in test['cat9'].value_counts(normalize=True).iteritems():\r\n",
        "    print(f'cate: {category}, pro: {proportion}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxFnE3oVzjhL"
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8KvxBR4zwuF"
      },
      "source": [
        "train.corr(method='pearson')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0r81J_U2lr7"
      },
      "source": [
        "#heatmap으로 상관관계를 표시\r\n",
        "import seaborn as sb\r\n",
        "plt.rcParams[\"figure.figsize\"] = (40,40)\r\n",
        "sb.heatmap(train.corr(),\r\n",
        "           annot = True, #실제 값 화면에 나타내기\r\n",
        "           cmap = 'Greens', #색상\r\n",
        "           vmin = -1, vmax=1 , #컬러차트 영역 -1 ~ +1\r\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7lmzZCLKpgJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4eivs28nlzb"
      },
      "source": [
        "#-----dnn---------\r\n",
        "cat_cols = train.columns[1:11]\r\n",
        "cat_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQkmqFiYB37n"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from collections import defaultdict\r\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler,RobustScaler\r\n",
        "class Preprocessor():\r\n",
        "    def __init__(self):\r\n",
        "        self.en_dic = None\r\n",
        "        self.standard_scaler = None\r\n",
        "        self.num_cols = None\r\n",
        "        self.cat_cols = None\r\n",
        "        # self.test_cats_onehot,self.test_onehot_cols = self.cats_onehot(test_cats_en)\r\n",
        "    def preprocess(self,data_df,train=True,combine_min_cats=False, add_pca_feats=False):\r\n",
        "\r\n",
        "        if train:\r\n",
        "            self.train_ids = data_df.loc[:, 'id']\r\n",
        "            train_cats = data_df.loc[:, data_df.dtypes == object]\r\n",
        "            self.cat_cols = train_cats.columns\r\n",
        "\r\n",
        "            if combine_min_cats:\r\n",
        "                self._find_minority_cats(train_cats)\r\n",
        "                train_cats = self._combine_minority_feats(train_cats)\r\n",
        "\r\n",
        "            self.en_dic = defaultdict(LabelEncoder)\r\n",
        "            train_cats_en = train_cats.apply(lambda x: self.en_dic[x.name].fit_transform(x))\r\n",
        "            tr_cats_onehot,tr_onehot_cols = self.cats_onehot(train_cats_en)\r\n",
        "            \r\n",
        "            train_num = data_df.loc[:,data_df.dtypes != object].drop(columns=['target','id'])\r\n",
        "            self.num_cols = train_num.columns\r\n",
        "            self.Standard_Scaler = StandardScaler()\r\n",
        "            train_num_scaler = self.Standard_Scaler.fit_transform(train_num)\r\n",
        "\r\n",
        "            if add_pca_feats:\r\n",
        "                pca_feats = self._return_num_pca(train_num_scaler)\r\n",
        "                X = pd.DataFrame(np.hstack((tr_cats_onehot,pca_feats)),columns=list(tr_onehot_cols)+list(self.pca_cols))\r\n",
        "            else:\r\n",
        "                X = pd.DataFrame(np.hstack((tr_cats_onehot,train_num_scaler)),columns = list(tr_onehot_cols)+list(self.num_cols))\r\n",
        "\r\n",
        "        else:\r\n",
        "            self.test_ids = data_df.loc[:,'id']\r\n",
        "            test_cats = data_df.loc[:, cat_cols]\r\n",
        "            if combine_min_cats:\r\n",
        "                self._find_minority_cats(test_cats)\r\n",
        "                test_cats = self._combine_minority_feats(test_cats)\r\n",
        "                \r\n",
        "            test_cats_en = test_cats.apply(lambda x: self.en_dic[x.name].fit_transform(x))\r\n",
        "            test_cats_onehot,test_onehot_cols = self.cats_onehot(test_cats_en)\r\n",
        "            test_num = data_df.loc[:,self.num_cols]\r\n",
        "            test_num_scaler = self.Standard_Scaler.fit_transform(test_num)\r\n",
        "\r\n",
        "            if add_pca_feats:\r\n",
        "                pca_feats = self._return_num_pca(test_num_scaler,train=False)\r\n",
        "\r\n",
        "                X = pd.DataFrame(np.hstack((test_cats_onehot,pca_feats)),columns = list(test_onehot_cols)+list(self.pca_cols))\r\n",
        "            \r\n",
        "            else:\r\n",
        "                X = pd.DataFrame(np.hstack((test_cats_onehot,test_num_scaler)),columns = list(test_onehot_cols)+list(self.num_cols))\r\n",
        "\r\n",
        "        return X\r\n",
        "\r\n",
        "    def cats_onehot(self,data_df):\r\n",
        "        self.cats_df = pd.get_dummies(data=data_df,columns=self.cat_cols, prefix= self.cat_cols)\r\n",
        "        self.cats_onehot_cols = self.cats_df.columns\r\n",
        "        return self.cats_df, self.cats_onehot_cols\r\n",
        "   \r\n",
        "    def _find_minority_cats(self, data_df):\r\n",
        "        self.composite_category = 'z'\r\n",
        "        self.threshold = 0.05\r\n",
        "        self.minority_col_dict = {}\r\n",
        "        self.minority_map_dic = {}\r\n",
        "        for feature in cat_cols:\r\n",
        "            self.minority_col_dict[feature] = []\r\n",
        "            self.minority_map_dic[feature] = {}\r\n",
        "            \r\n",
        "            for category,proportion in data_df[feature].value_counts(normalize=True).iteritems():\r\n",
        "                if proportion < self.threshold:\r\n",
        "                    self.minority_col_dict[feature].append(category)\r\n",
        "                    self.minority_map_dic[feature] = { x : self.composite_category for x in self.minority_col_dict[feature]}\r\n",
        "        return self.minority_map_dic, self.minority_col_dict\r\n",
        "    \r\n",
        "    def _combine_minority_feats(self, data_df, replace = False):\r\n",
        "        new_df = data_df.copy()\r\n",
        "        for feat in self.cat_cols:\r\n",
        "            col_label = f\"{feat}\" if replace else f\"{feat}_new\"\r\n",
        "            new_df[feat] = new_df[feat].replace(self.minority_map_dic[feat])\r\n",
        "        return new_df\r\n",
        "\r\n",
        "    def _return_num_pca(self,num_df,train=True):\r\n",
        "        self.n_components = 0.85\r\n",
        "        if train:\r\n",
        "            self.pca = PCA(n_components = self.n_components)\r\n",
        "            \r\n",
        "            num_rd = self.pca.fit_transform(num_df)\r\n",
        "            print(f'pca_explain: {self.pca.explained_variance_ratio_}')\r\n",
        "            self.pca_cols = [f'pca_{x}' for x in range(num_rd.shape[1])]\r\n",
        "\r\n",
        "        else:\r\n",
        "            num_rd = self.pca.transform(num_df)\r\n",
        "\r\n",
        "            self.pca_cols = [f'pca_{x}' for x in range(num_rd.shape[1])]\r\n",
        "        \r\n",
        "        return pd.DataFrame(num_rd, columns = self.pca_cols)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkkYMaIZeFAP"
      },
      "source": [
        "data_proc = Preprocessor()\r\n",
        "X = data_proc.preprocess(train, combine_min_cats=False, add_pca_feats=True)\r\n",
        "y = train.loc[:, 'target']\r\n",
        "X_test = data_proc.preprocess(test,train=False,combine_min_cats=False,add_pca_feats=True)\r\n",
        "pd.set_option('display.max_columns', 500)\r\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBQ4qAZP8DE3"
      },
      "source": [
        "X_test['cat6_7'] = 0\r\n",
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKFPXC-PydhK"
      },
      "source": [
        "X_test.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md3ZoYKJyiMS"
      },
      "source": [
        "X_test = X_test[['cat0_0', 'cat0_1', 'cat1_0', 'cat1_1', 'cat2_0', 'cat2_1', 'cat3_0',\r\n",
        "       'cat3_1', 'cat3_2', 'cat3_3', 'cat4_0', 'cat4_1', 'cat4_2', 'cat4_3',\r\n",
        "       'cat5_0', 'cat5_1', 'cat5_2', 'cat5_3', 'cat6_0', 'cat6_1', 'cat6_2',\r\n",
        "       'cat6_3', 'cat6_4', 'cat6_5', 'cat6_6', 'cat6_7', 'cat7_0', 'cat7_1', 'cat7_2',\r\n",
        "       'cat7_3', 'cat7_4', 'cat7_5', 'cat7_6', 'cat7_7', 'cat8_0', 'cat8_1',\r\n",
        "       'cat8_2', 'cat8_3', 'cat8_4', 'cat8_5', 'cat8_6', 'cat9_0', 'cat9_1',\r\n",
        "       'cat9_2', 'cat9_3', 'cat9_4', 'cat9_5', 'cat9_6', 'cat9_7', 'cat9_8',\r\n",
        "       'cat9_9', 'cat9_10', 'cat9_11', 'cat9_12', 'cat9_13', 'cat9_14',\r\n",
        "       'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7',\r\n",
        "       'pca_8']]\r\n",
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF0hVhrU0X2j"
      },
      "source": [
        "# features_drop= ['cat6_7']\r\n",
        "X_test = X_test.drop(X_test['cat6_7'],axis=1)\r\n",
        "X = X.drop(X['cat6_7'],axis=1)\r\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSsfwF1b1t5w"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1avOUTiG1zSh"
      },
      "source": [
        "X = X.drop('cat6_7',axis=1)\r\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yETvp7Y3DTtL"
      },
      "source": [
        "print(f'xshape: {X.shape} \\n yshape: {y.shape} \\n X_test: {X_test.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9-ubf6SBuUg"
      },
      "source": [
        "for tr,te in zip(X.columns,X_test.columns):\r\n",
        "    if tr != te:\r\n",
        "        print(f'tr: {tr} \\n te: {te}')\r\n",
        "    else:\r\n",
        "        print((\"success\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcznHP4s2JKn"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\r\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12)\r\n",
        "# X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QSespcL2VH-"
      },
      "source": [
        "import keras\r\n",
        "from keras.layers import Dense, Embedding, Flatten, LSTM, GRU, \\\r\n",
        "        SpatialDropout1D, Bidirectional, Conv1D, MaxPooling1D, BatchNormalization\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras import models\r\n",
        "from keras import layers\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "epoch = 100\r\n",
        "def aggregate_lr_rate(epoch, lr):\r\n",
        "\r\n",
        "    if epoch < 20:\r\n",
        "        return lr\r\n",
        "    else:\r\n",
        "        return lr * tf.math.exp(-0.1)\r\n",
        "\r\n",
        "def dnn_model(lr=aggregate_lr_rate(epoch,0.001), input_feat_dim=X.shape[1]):\r\n",
        "    \"\"\" Create a Deep NN for regression that supports monte carlo dropout\r\n",
        "        and uses Batch Norm \"\"\"\r\n",
        "    model = Sequential()\r\n",
        "    \r\n",
        "    model.add(layers.Dense(256, activation='relu', input_shape=(input_feat_dim,), \r\n",
        "                           kernel_initializer='he_normal'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(layers.Dropout(0.2))\r\n",
        "\r\n",
        "    model.add(layers.Dense(128, activation='relu', kernel_initializer='he_normal'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(layers.Dropout(0.2))\r\n",
        "\r\n",
        "    model.add(layers.Dense(50, activation='relu', kernel_initializer='he_normal'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(layers.Dropout(0.2))\r\n",
        "        \r\n",
        "    # regression output layer - no activation\r\n",
        "    model.add(layers.Dense(1))\r\n",
        "        \r\n",
        "    model.compile(optimizer=keras.optimizers.adam(lr=lr), \r\n",
        "                  loss='mse', metrics=['mse'])\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsPW1mgMfOWC"
      },
      "source": [
        "tr_data = [X,y]\r\n",
        "tr_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUOABvdHZlCM"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "k=5\r\n",
        "num_val_samples = len(X)//k\r\n",
        "num_epoch = 100\r\n",
        "all_scores = []\r\n",
        "np.random.shuffle(tr_data)\r\n",
        "for i in range(k):\r\n",
        "\r\n",
        "    print(f'처리중인 폴드 : {i+1}')\r\n",
        "    # np.random.shuffle(tr_data)\r\n",
        "\r\n",
        "    val_data = X[i*num_val_samples: (i+1)*num_val_samples]\r\n",
        "    val_targets = y[i*num_val_samples: (i+1)*num_val_samples]\r\n",
        "\r\n",
        "    partition_tr = np.concatenate([X[:i*num_val_samples],\r\n",
        "                              X[(i+1)*num_val_samples: ]], axis=0)\r\n",
        "    partition_tr_target = np.concatenate([y[:i*num_val_samples],\r\n",
        "                              y[(i+1)*num_val_samples: ]], axis=0)\r\n",
        "    model_1 = dnn_model()\r\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=20)\r\n",
        "    history = model_1.fit(partition_tr, partition_tr_target, epochs=num_epoch, batch_size=256, \r\n",
        "                      callbacks=[early_stop],validation_data=(val_data, val_targets))\r\n",
        "    val_mse = history.history['val_mse']   #evaluate 함수는 검증결과를 출력할수있다\r\n",
        "    all_scores.append(val_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtsYAnbesC3r"
      },
      "source": [
        "# np.mean(all_scores)\r\n",
        "print(all_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICKxhNMNtzJQ"
      },
      "source": [
        "test_pred = model_1.predict(X_test)\r\n",
        "test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t31bNkZ8uCFi"
      },
      "source": [
        "# save submission in csv format\r\n",
        "import datetime\r\n",
        "timetoday = datetime.datetime.today()\r\n",
        "today = [timetoday.day,timetoday.hour,timetoday.minute]\r\n",
        "submission_df = pd.read_csv('/content/drive/My Drive//tabular/sample_submission.csv')\r\n",
        "submission_df['target'] = test_pred\r\n",
        "submission_df.to_csv(f'submission_tabular_{today}.csv',index=False)\r\n",
        "!ls\r\n",
        "from google.colab import files\r\n",
        "files.download(f'submission_tabular_{today}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx60Di4l5Kl6"
      },
      "source": [
        "def plot_results(history):\r\n",
        "    metric = 'mse'\r\n",
        "    figsize = (12,5)\r\n",
        "\r\n",
        "    tr_loss = history.history['loss']\r\n",
        "    val_loss = history.history['val_loss']\r\n",
        "    epochs = range(len(tr_loss))\r\n",
        "\r\n",
        "    fig = plt.figure(figsize=figsize)\r\n",
        "\r\n",
        "    ax = fig.add_subplot(1,1,1)\r\n",
        "    plt.plot(epochs,tr_loss,marker='o',label = 'train')\r\n",
        "    plt.plot(epochs,val_loss,marker='x',label = 'val')\r\n",
        "\r\n",
        "    plt.title('train/val')\r\n",
        "    ax.set_ylabel('loss')\r\n",
        "    ax.set_xlabel('epoch')\r\n",
        "    plt.legend(loc='best')\r\n",
        "    plt.show()\r\n",
        "plot_results(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJYz4OEU9kj7"
      },
      "source": [
        "print(history.history('loss'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDdGBHfB-G9-"
      },
      "source": [
        "def pred_mc(model, test_inputs):\r\n",
        "    \r\n",
        "    # pred_probs = [model.predict(test_inputs) for samples in range(n_samples)]\r\n",
        "    pred_probs = model.predict(test_inputs)\r\n",
        "    return np.mean(pred_probs, axis=0)\r\n",
        "\r\n",
        "test_preds_1 = pred_mc(model_1, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjx_J1m_JmPw"
      },
      "source": [
        "# save submission in csv format\r\n",
        "submission_df = pd.read_csv('/content/drive/My Drive//tabular/sample_submission.csv')\r\n",
        "submission_df['target'] = test_preds_1\r\n",
        "submission_df.to_csv('submission_tabular.csv',index=False)\r\n",
        "!ls\r\n",
        "from google.colab import files\r\n",
        "files.download('submission_tabular.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn_VQhIxOzrL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0jKrWgexXUN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}