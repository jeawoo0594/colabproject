{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tabular_ensemble.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOxwxOvDYnQQE6K/Ow3Qcq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeawoo0594/colabproject/blob/main/tabular_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa05EdJKsjjx"
      },
      "source": [
        "# Basic Imports \r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Plotting \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import plotly.express as px\r\n",
        "import seaborn as sns\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Preprocessing\r\n",
        "from sklearn.model_selection import train_test_split, KFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "# Metrics \r\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\r\n",
        "\r\n",
        "# ML Models\r\n",
        "import lightgbm as lgb\r\n",
        "from lightgbm import LGBMRegressor \r\n",
        "import xgboost as xg \r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn import svm\r\n",
        "\r\n",
        "# Ignore Warnings \r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "#drive uproad\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu2E6TthatMO"
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4M1DzLbbAAG"
      },
      "source": [
        "!pip install Bayesian-Optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YpME93AaqEd"
      },
      "source": [
        "# Feature Importance \r\n",
        "import shap\r\n",
        "\r\n",
        "# Model Tuning \r\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2STjFIFsprO"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive//tabular/train.csv')\r\n",
        "test = pd.read_csv('/content/drive/My Drive//tabular/test.csv')\r\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI2nvvFNtI_j"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "import timeit\r\n",
        "\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  print(\r\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\r\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\r\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "\r\n",
        "def cpu():\r\n",
        "  with tf.device('/cpu:0'):\r\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\r\n",
        "    return tf.math.reduce_sum(net_cpu)\r\n",
        "\r\n",
        "def gpu():\r\n",
        "  with tf.device('/device:GPU:0'):\r\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\r\n",
        "    return tf.math.reduce_sum(net_gpu)\r\n",
        "  \r\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\r\n",
        "cpu()\r\n",
        "gpu()\r\n",
        "\r\n",
        "# Run the op several times.\r\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\r\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\r\n",
        "print('CPU (s):')\r\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\r\n",
        "print(cpu_time)\r\n",
        "print('GPU (s):')\r\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\r\n",
        "print(gpu_time)\r\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PpcCGcqtLrq"
      },
      "source": [
        "train.describe()\r\n",
        "train.isnull().sum()\r\n",
        "train.corr(method='pearson')\r\n",
        "#heatmap으로 상관관계를 표시\r\n",
        "import seaborn as sb\r\n",
        "plt.rcParams[\"figure.figsize\"] = (40,40)\r\n",
        "sb.heatmap(train.corr(),\r\n",
        "           annot = True, #실제 값 화면에 나타내기\r\n",
        "           cmap = 'Greens', #색상\r\n",
        "           vmin = -1, vmax=1 , #컬러차트 영역 -1 ~ +1\r\n",
        "          )\r\n",
        "#선형회귀\r\n",
        "import statsmodels.api as sm\r\n",
        "multi_model = sm.OLS(y_train, x_train)\r\n",
        "fitted_multi_model = multi_model.fit()\r\n",
        "fitted_multi_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvzx89NlthEV"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "from collections import defaultdict\r\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler,RobustScaler\r\n",
        "class Preprocessor():\r\n",
        "    def __init__(self):\r\n",
        "        self.en_dic = None\r\n",
        "        self.standard_scaler = None\r\n",
        "        self.num_cols = None\r\n",
        "        self.cat_cols = None\r\n",
        "        # self.test_cats_onehot,self.test_onehot_cols = self.cats_onehot(test_cats_en)\r\n",
        "    def preprocess(self,data_df,train=True,combine_min_cats=False, add_pca_feats=False):\r\n",
        "\r\n",
        "        if train:\r\n",
        "            self.train_ids = data_df.loc[:, 'id']\r\n",
        "            train_cats = data_df.loc[:, data_df.dtypes == object]\r\n",
        "            self.cat_cols = train_cats.columns\r\n",
        "\r\n",
        "            if combine_min_cats:\r\n",
        "                self._find_minority_cats(train_cats)\r\n",
        "                train_cats = self._combine_minority_feats(train_cats)\r\n",
        "\r\n",
        "            self.en_dic = defaultdict(LabelEncoder)\r\n",
        "            train_cats_en = train_cats.apply(lambda x: self.en_dic[x.name].fit_transform(x))\r\n",
        "            tr_cats_onehot,tr_onehot_cols = self.cats_onehot(train_cats_en)\r\n",
        "            \r\n",
        "            train_num = data_df.loc[:,data_df.dtypes != object].drop(columns=['target','id'])\r\n",
        "            self.num_cols = train_num.columns\r\n",
        "            self.Robust_Scaler = RobustScaler()\r\n",
        "            train_num_scaler = self.Robust_Scaler.fit_transform(train_num)\r\n",
        "\r\n",
        "            if add_pca_feats:\r\n",
        "                pca_feats = self._return_num_pca(train_num_scaler)\r\n",
        "                X = pd.DataFrame(np.hstack((train_cats_en,pca_feats)),columns=list(train_cats_en)+list(self.pca_cols))\r\n",
        "            else:\r\n",
        "                X = pd.DataFrame(np.hstack((train_cats_en,train_num_scaler)),columns = list(train_cats_en)+list(self.num_cols))\r\n",
        "\r\n",
        "        else:\r\n",
        "            self.test_ids = data_df.loc[:,'id']\r\n",
        "            test_cats = data_df.loc[:, self.cat_cols]\r\n",
        "            if combine_min_cats:\r\n",
        "                self._find_minority_cats(test_cats)\r\n",
        "                test_cats = self._combine_minority_feats(test_cats)\r\n",
        "                \r\n",
        "            test_cats_en = test_cats.apply(lambda x: self.en_dic[x.name].fit_transform(x))\r\n",
        "            test_cats_onehot,test_onehot_cols = self.cats_onehot(test_cats_en)\r\n",
        "            test_num = data_df.loc[:,self.num_cols]\r\n",
        "            test_num_scaler = self.Robust_Scaler.fit_transform(test_num)\r\n",
        "\r\n",
        "            if add_pca_feats:\r\n",
        "                pca_feats = self._return_num_pca(test_num_scaler,train=False)\r\n",
        "\r\n",
        "                X = pd.DataFrame(np.hstack((test_cats_en,pca_feats)),columns = list(test_cats_en)+list(self.pca_cols))\r\n",
        "            \r\n",
        "            else:\r\n",
        "                X = pd.DataFrame(np.hstack((test_cats_en,test_num_scaler)),columns = list(test_cats_en)+list(self.num_cols))\r\n",
        "\r\n",
        "        return X\r\n",
        "\r\n",
        "    def cats_onehot(self,data_df):\r\n",
        "        self.cats_df = pd.get_dummies(data=data_df,columns=self.cat_cols, prefix= self.cat_cols)\r\n",
        "        self.cats_onehot_cols = self.cats_df.columns\r\n",
        "        return self.cats_df, self.cats_onehot_cols\r\n",
        "   \r\n",
        "    def _find_minority_cats(self, data_df):\r\n",
        "        self.composite_category = 'z'\r\n",
        "        self.threshold = 0.05\r\n",
        "        self.minority_col_dict = {}\r\n",
        "        self.minority_map_dic = {}\r\n",
        "        for feature in self.cat_cols:\r\n",
        "            self.minority_col_dict[feature] = []\r\n",
        "            self.minority_map_dic[feature] = {}\r\n",
        "            \r\n",
        "            for category,proportion in data_df[feature].value_counts(normalize=True).iteritems():\r\n",
        "                if proportion < self.threshold:\r\n",
        "                    self.minority_col_dict[feature].append(category)\r\n",
        "                    self.minority_map_dic[feature] = { x : self.composite_category for x in self.minority_col_dict[feature]}\r\n",
        "        return self.minority_map_dic, self.minority_col_dict\r\n",
        "    \r\n",
        "    def _combine_minority_feats(self, data_df, replace = False):\r\n",
        "        new_df = data_df.copy()\r\n",
        "        for feat in self.cat_cols:\r\n",
        "            col_label = f\"{feat}\" if replace else f\"{feat}_new\"\r\n",
        "            new_df[feat] = new_df[feat].replace(self.minority_map_dic[feat])\r\n",
        "        return new_df\r\n",
        "\r\n",
        "    def _return_num_pca(self,num_df,train=True):\r\n",
        "        self.n_components = 0.85\r\n",
        "        if train:\r\n",
        "            self.pca = PCA(n_components = self.n_components)\r\n",
        "            \r\n",
        "            num_rd = self.pca.fit_transform(num_df)\r\n",
        "            print(f'pca_explain: {self.pca.explained_variance_ratio_}')\r\n",
        "            self.pca_cols = [f'pca_{x}' for x in range(num_rd.shape[1])]\r\n",
        "\r\n",
        "        else:\r\n",
        "            num_rd = self.pca.transform(num_df)\r\n",
        "\r\n",
        "            self.pca_cols = [f'pca_{x}' for x in range(num_rd.shape[1])]\r\n",
        "        \r\n",
        "        return pd.DataFrame(num_rd, columns = self.pca_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS577vQ0tkO0"
      },
      "source": [
        "data_proc = Preprocessor()\r\n",
        "X = data_proc.preprocess(train, combine_min_cats=False, add_pca_feats=True)\r\n",
        "y = train.loc[:, 'target']\r\n",
        "X_test = data_proc.preprocess(test,train=False,combine_min_cats=False, add_pca_feats=True)\r\n",
        "pd.set_option('display.max_columns', 500)\r\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FLGvxwhtmnM"
      },
      "source": [
        "# ad_x = X.drop(X.columns[10:24],axis=1)\r\n",
        "# ad_x\r\n",
        "# ad_x = ad_x.drop(['pca_6'],axis=1)\r\n",
        "# ad_test = X_test.drop(X_test.columns[10:24],axis=1)\r\n",
        "# ad_test = ad_test.drop(['pca_6'],axis=1)\r\n",
        "# ad_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6OuNbn0Z_i8"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0tV8MUet3P2"
      },
      "source": [
        "def search_best_param(x_train,y_train,cat_features):\r\n",
        "    SEED = 123\r\n",
        "    cat_features = x_train.columns[0:10]\r\n",
        "    train = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\r\n",
        "\r\n",
        "    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \r\n",
        "                lambda_l1, lambda_l2, min_child_weight):\r\n",
        "        params = {'boosting_type': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'verbose': -1,\r\n",
        "                  'early_stopping_round':100}\r\n",
        "        params['max_depth'] = int(round(max_depth))\r\n",
        "        params[\"num_leaves\"] = int(round(num_leaves))\r\n",
        "        params[\"n_estimators\"] = int(round(n_estimators))\r\n",
        "        params['learning_rate'] = learning_rate\r\n",
        "        params['subsample'] = subsample\r\n",
        "        params['colsample_bytree'] = colsample_bytree\r\n",
        "        params['lambda_l1'] = max(lambda_l1, 0)\r\n",
        "        params['lambda_l2'] = max(lambda_l2, 0)\r\n",
        "        params['min_child_weight'] = int(round(min_child_weight))\r\n",
        "        \r\n",
        "        score = lgb.cv(params, train, nfold=5, seed=123, stratified=False, verbose_eval =False, metrics=['rmse'])\r\n",
        "        \r\n",
        "        return -np.min(score['rmse-mean'])\r\n",
        "\r\n",
        "    lightGBM_BO = BayesianOptimization(lightGBM_CV, {\r\n",
        "                                          'max_depth': (5, 50),\r\n",
        "                                          'num_leaves': (20, 100),\r\n",
        "                                          'n_estimators': (1000, 30000),\r\n",
        "                                          'learning_rate': (0.05, 0.3),\r\n",
        "                                          'subsample': (0.7, 0.8),\r\n",
        "                                          'colsample_bytree' :(0.5, 0.99),\r\n",
        "                                          'lambda_l1': (0, 5),\r\n",
        "                                          'lambda_l2': (0, 3),\r\n",
        "                                          'min_child_weight': (2, 50) \r\n",
        "                                      },\r\n",
        "                                       random_state = SEED,\r\n",
        "                                       verbose = -1)\r\n",
        "    np.random.seed(SEED)\r\n",
        "    lightGBM_BO.maximize(init_points=5, n_iter=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAlnXYGIt55D"
      },
      "source": [
        "# save submission in csv format\r\n",
        "submission_df = pd.read_csv('/content/drive/My Drive//tabular/sample_submission.csv')\r\n",
        "submission_df['target'] = preds\r\n",
        "submission_df.to_csv('submission_tabular_ensemble.csv',index=False)\r\n",
        "!ls\r\n",
        "from google.colab import files\r\n",
        "files.download('submission_tabular_ensemble.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7swPAj8ntIhz"
      },
      "source": [
        ""
      ]
    }
  ]
}