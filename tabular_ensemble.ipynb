{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tabular_ensemble.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOA2kjk8sicufmvV+qtdkvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeawoo0594/colabproject/blob/main/tabular_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa05EdJKsjjx"
      },
      "source": [
        "import seaborn as sns\r\n",
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib.offsetbox import AnchoredText\r\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2STjFIFsprO"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive//tabular/train.csv')\r\n",
        "test = pd.read_csv('/content/drive/My Drive//tabular/test.csv')\r\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI2nvvFNtI_j"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "import tensorflow as tf\r\n",
        "import timeit\r\n",
        "\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  print(\r\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\r\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\r\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "\r\n",
        "def cpu():\r\n",
        "  with tf.device('/cpu:0'):\r\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\r\n",
        "    return tf.math.reduce_sum(net_cpu)\r\n",
        "\r\n",
        "def gpu():\r\n",
        "  with tf.device('/device:GPU:0'):\r\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\r\n",
        "    return tf.math.reduce_sum(net_gpu)\r\n",
        "  \r\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\r\n",
        "cpu()\r\n",
        "gpu()\r\n",
        "\r\n",
        "# Run the op several times.\r\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\r\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\r\n",
        "print('CPU (s):')\r\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\r\n",
        "print(cpu_time)\r\n",
        "print('GPU (s):')\r\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\r\n",
        "print(gpu_time)\r\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PpcCGcqtLrq"
      },
      "source": [
        "train.describe()\r\n",
        "train.isnull().sum()\r\n",
        "train.corr(method='pearson')\r\n",
        "#heatmap으로 상관관계를 표시\r\n",
        "import seaborn as sb\r\n",
        "plt.rcParams[\"figure.figsize\"] = (40,40)\r\n",
        "sb.heatmap(train.corr(),\r\n",
        "           annot = True, #실제 값 화면에 나타내기\r\n",
        "           cmap = 'Greens', #색상\r\n",
        "           vmin = -1, vmax=1 , #컬러차트 영역 -1 ~ +1\r\n",
        "          )\r\n",
        "#선형회귀\r\n",
        "import statsmodels.api as sm\r\n",
        "multi_model = sm.OLS(y_train, x_train)\r\n",
        "fitted_multi_model = multi_model.fit()\r\n",
        "fitted_multi_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvzx89NlthEV"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from collections import defaultdict\r\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler,RobustScaler\r\n",
        "class Preprocessor():\r\n",
        "    def __init__(self):\r\n",
        "        self.en_dic = None\r\n",
        "        self.standard_scaler = None\r\n",
        "        self.num_cols = None\r\n",
        "        self.cat_cols = None\r\n",
        "        # self.test_cats_onehot,self.test_onehot_cols = self.cats_onehot(test_cats_en)\r\n",
        "    def preprocess(self,data_df,train=True,combine_min_cats=False, add_pca_feats=False):\r\n",
        "\r\n",
        "        if train:\r\n",
        "            self.train_ids = data_df.loc[:, 'id']\r\n",
        "            train_cats = data_df.loc[:, data_df.dtypes == object]\r\n",
        "            self.cat_cols = train_cats.columns\r\n",
        "\r\n",
        "            if combine_min_cats:\r\n",
        "                self._find_minority_cats(train_cats)\r\n",
        "                train_cats = self._combine_minority_feats(train_cats)\r\n",
        "\r\n",
        "            self.en_dic = defaultdict(LabelEncoder)\r\n",
        "            train_cats_en = train_cats.apply(lambda x: self.en_dic[x.name].fit_transform(x))\r\n",
        "            tr_cats_onehot,tr_onehot_cols = self.cats_onehot(train_cats_en)\r\n",
        "            \r\n",
        "            train_num = data_df.loc[:,data_df.dtypes != object].drop(columns=['target','id'])\r\n",
        "            self.num_cols = train_num.columns\r\n",
        "            self.Robust_Scaler = RobustScaler()\r\n",
        "            train_num_scaler = self.Robust_Scaler.fit_transform(train_num)\r\n",
        "\r\n",
        "            if add_pca_feats:\r\n",
        "                pca_feats = self._return_num_pca(train_num_scaler)\r\n",
        "                X = pd.DataFrame(np.hstack((tr_cats_onehot,pca_feats)),columns=list(tr_onehot_cols)+list(self.pca_cols))\r\n",
        "            else:\r\n",
        "                X = pd.DataFrame(np.hstack((tr_cats_onehot,train_num_scaler)),columns = list(tr_onehot_cols)+list(self.num_cols))\r\n",
        "\r\n",
        "        else:\r\n",
        "            self.test_ids = data_df.loc[:,'id']\r\n",
        "            test_cats = data_df.loc[:, cat_cols]\r\n",
        "            if combine_min_cats:\r\n",
        "                self._find_minority_cats(test_cats)\r\n",
        "                test_cats = self._combine_minority_feats(test_cats)\r\n",
        "                \r\n",
        "            test_cats_en = test_cats.apply(lambda x: self.en_dic[x.name].fit_transform(x))\r\n",
        "            test_cats_onehot,test_onehot_cols = self.cats_onehot(test_cats_en)\r\n",
        "            test_num = data_df.loc[:,self.num_cols]\r\n",
        "            test_num_scaler = self.Robust_Scaler.fit_transform(test_num)\r\n",
        "\r\n",
        "            if add_pca_feats:\r\n",
        "                pca_feats = self._return_num_pca(test_num_scaler,train=False)\r\n",
        "\r\n",
        "                X = pd.DataFrame(np.hstack((test_cats_onehot,pca_feats)),columns = list(test_onehot_cols)+list(self.pca_cols))\r\n",
        "            \r\n",
        "            else:\r\n",
        "                X = pd.DataFrame(np.hstack((test_cats_onehot,test_num_scaler)),columns = list(test_onehot_cols)+list(self.num_cols))\r\n",
        "\r\n",
        "        return X\r\n",
        "\r\n",
        "    def cats_onehot(self,data_df):\r\n",
        "        self.cats_df = pd.get_dummies(data=data_df,columns=self.cat_cols, prefix= self.cat_cols)\r\n",
        "        self.cats_onehot_cols = self.cats_df.columns\r\n",
        "        return self.cats_df, self.cats_onehot_cols\r\n",
        "   \r\n",
        "    def _find_minority_cats(self, data_df):\r\n",
        "        self.composite_category = 'z'\r\n",
        "        self.threshold = 0.05\r\n",
        "        self.minority_col_dict = {}\r\n",
        "        self.minority_map_dic = {}\r\n",
        "        for feature in cat_cols:\r\n",
        "            self.minority_col_dict[feature] = []\r\n",
        "            self.minority_map_dic[feature] = {}\r\n",
        "            \r\n",
        "            for category,proportion in data_df[feature].value_counts(normalize=True).iteritems():\r\n",
        "                if proportion < self.threshold:\r\n",
        "                    self.minority_col_dict[feature].append(category)\r\n",
        "                    self.minority_map_dic[feature] = { x : self.composite_category for x in self.minority_col_dict[feature]}\r\n",
        "        return self.minority_map_dic, self.minority_col_dict\r\n",
        "    \r\n",
        "    def _combine_minority_feats(self, data_df, replace = False):\r\n",
        "        new_df = data_df.copy()\r\n",
        "        for feat in self.cat_cols:\r\n",
        "            col_label = f\"{feat}\" if replace else f\"{feat}_new\"\r\n",
        "            new_df[feat] = new_df[feat].replace(self.minority_map_dic[feat])\r\n",
        "        return new_df\r\n",
        "\r\n",
        "    def _return_num_pca(self,num_df,train=True):\r\n",
        "        self.n_components = 0.8\r\n",
        "        if train:\r\n",
        "            self.pca = PCA(n_components = self.n_components)\r\n",
        "            \r\n",
        "            num_rd = self.pca.fit_transform(num_df)\r\n",
        "            print(f'pca_explain: {self.pca.explained_variance_ratio_}')\r\n",
        "            self.pca_cols = [f'pca_{x}' for x in range(num_rd.shape[1])]\r\n",
        "\r\n",
        "        else:\r\n",
        "            num_rd = self.pca.transform(num_df)\r\n",
        "\r\n",
        "            self.pca_cols = [f'pca_{x}' for x in range(num_rd.shape[1])]\r\n",
        "        \r\n",
        "        return pd.DataFrame(num_rd, columns = self.pca_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS577vQ0tkO0"
      },
      "source": [
        "data_proc = Preprocessor()\r\n",
        "X = data_proc.preprocess(train, combine_min_cats=True, add_pca_feats=True)\r\n",
        "y = train.loc[:, 'target']\r\n",
        "X_test = data_proc.preprocess(test,train=False,combine_min_cats=True,add_pca_feats=True)\r\n",
        "pd.set_option('display.max_columns', 500)\r\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FLGvxwhtmnM"
      },
      "source": [
        "ad_x = X.drop(X.columns[10:24],axis=1)\r\n",
        "ad_x\r\n",
        "ad_x = ad_x.drop(['pca_6'],axis=1)\r\n",
        "ad_test = X_test.drop(X_test.columns[10:24],axis=1)\r\n",
        "ad_test = ad_test.drop(['pca_6'],axis=1)\r\n",
        "ad_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0tV8MUet3P2"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "# ------------------------------------------------------------------------------\r\n",
        "# Parameters\r\n",
        "# ------------------------------------------------------------------------------\r\n",
        "N_FOLDS = 5\r\n",
        "N_ESTIMATORS = 30000\r\n",
        "SEED = 1234\r\n",
        "BAGGING_SEED = 481\r\n",
        "\r\n",
        "# ------------------------------------------------------------------------------\r\n",
        "# LightGBM: training and inference\r\n",
        "# ------------------------------------------------------------------------------\r\n",
        "lgb_params = {'random_state': SEED,\r\n",
        "          'metric': 'rmse',\r\n",
        "          'n_estimators': N_ESTIMATORS,\r\n",
        "          'n_jobs': -1,\r\n",
        "        #   'cat_feature': [x for x in range(len(cat_cols))],\r\n",
        "          'bagging_seed': SEED,\r\n",
        "          'feature_fraction_seed': SEED,\r\n",
        "          'learning_rate': 0.0011323124124,\r\n",
        "          'max_depth': 99,\r\n",
        "          'num_leaves': 63,\r\n",
        "          'reg_alpha': 9.562925363678952,\r\n",
        "          'reg_lambda': 9.355810045480153,\r\n",
        "          'colsample_bytree': 0.2256038826485174,\r\n",
        "          'min_child_samples': 290,\r\n",
        "          'subsample_freq': 1,\r\n",
        "          'subsample': 0.8805303688019942,\r\n",
        "          'max_bin': 882,\r\n",
        "          'min_data_per_group': 127\r\n",
        "        #   'cat_smooth': 96,\r\n",
        "        #   'cat_l2': 19\r\n",
        "          }\r\n",
        "\r\n",
        "ensemble_params = {\r\n",
        "    \"lgb\" : lgb_params,\r\n",
        "    'xgb': {\r\n",
        "        'random_state': SEED,\r\n",
        "        'max_depth': 13,\r\n",
        "        'learning_rate': 0.0011323124124,\r\n",
        "        'gamma': 3.5746731812451156,\r\n",
        "        'min_child_weight': 564,\r\n",
        "        'n_estimators': 8000,\r\n",
        "        'colsample_bytree': 0.5015940592112956,\r\n",
        "        'subsample': 0.6839489639112909,\r\n",
        "        'reg_lambda': 18.085502002853246,\r\n",
        "        'reg_alpha': 0.17532087359570606,\r\n",
        "        'objective': 'reg:squarederror',\r\n",
        "        'tree_method': 'gpu_hist',\r\n",
        "        'eval_metric': 'rmse',\r\n",
        "        'n_jobs': -1\r\n",
        "    }\r\n",
        "    # 'cat': {\r\n",
        "    #     'random_state': SEED,\r\n",
        "    #     'depth': 3.0,\r\n",
        "    #     'fold_len_multiplier': 1.1425259013471902,\r\n",
        "    #     'l2_leaf_reg': 7.567589781752637,\r\n",
        "    #     'leaf_estimation_backtracking': 'AnyImprovement',\r\n",
        "    #     'learning_rate': 0.25121635918496565,\r\n",
        "    #     'max_bin': 107.0,\r\n",
        "    #     'min_data_in_leaf': 220.0,\r\n",
        "    #     'random_strength': 3.2658690042589726,\r\n",
        "    #     'n_estimators': 8000,\r\n",
        "    #     'eval_metric': 'RMSE',\r\n",
        "    # }\r\n",
        "}\r\n",
        "colums = ad_x.columns\r\n",
        "preds = np.zeros(ad_test.shape[0])\r\n",
        "k_fold = KFold(n_splits=N_FOLDS, random_state=122, shuffle=True)\r\n",
        "rmse = []\r\n",
        "n = 0\r\n",
        "\r\n",
        "for trn_idx, test_idx in k_fold.split(ad_x[colums], y):\r\n",
        "\r\n",
        "    X_tr, X_val=ad_x[colums].iloc[trn_idx], ad_x[colums].iloc[test_idx]\r\n",
        "    y_tr, y_val=y.iloc[trn_idx], y.iloc[test_idx]\r\n",
        "\r\n",
        "    model = EnsembleModel(ensemble_params)\r\n",
        "\r\n",
        "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=1)\r\n",
        "\r\n",
        "    preds += model.predict(ad_test[colums], weights=[1.0, 1.0]) / k_fold.n_splits\r\n",
        "    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\r\n",
        "    \r\n",
        "    print(f\"Fold {n+1}, RMSE: {rmse[n]}\")\r\n",
        "    n += 1\r\n",
        "\r\n",
        "    \r\n",
        "print(\"Mean RMSE: \", np.mean(rmse))\r\n",
        "# end_time = time.time() - since\r\n",
        "# print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
        "#         end_time // 60, end_time % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAlnXYGIt55D"
      },
      "source": [
        "# save submission in csv format\r\n",
        "submission_df = pd.read_csv('/content/drive/My Drive//tabular/sample_submission.csv')\r\n",
        "submission_df['target'] = preds\r\n",
        "submission_df.to_csv('submission_tabular_ensemble.csv',index=False)\r\n",
        "!ls\r\n",
        "from google.colab import files\r\n",
        "files.download('submission_tabular_ensemble.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7swPAj8ntIhz"
      },
      "source": [
        ""
      ]
    }
  ]
}